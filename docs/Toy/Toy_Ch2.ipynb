{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56341c66",
   "metadata": {},
   "source": [
    "# Chapter 2: Emitting Basic MLIR\n",
    "\n",
    "Now that we're familiar with our language and the AST, let's see how MLIR can\n",
    "help to compile Toy.\n",
    "\n",
    "## Introduction: Multi-Level Intermediate Representation\n",
    "\n",
    "Other compilers, like LLVM (see the\n",
    "[Kaleidoscope tutorial](https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/index.html)),\n",
    "offer a fixed set of predefined types and (usually *low-level* / RISC-like)\n",
    "instructions. It is up to the frontend for a given language to perform any\n",
    "language-specific type-checking, analysis, or transformation before emitting\n",
    "LLVM IR. For example, Clang will use its AST to perform not only static analysis\n",
    "but also transformations, such as C++ template instantiation through AST cloning\n",
    "and rewrite. Finally, languages with construction at a higher-level than C/C++\n",
    "may require non-trivial lowering from their AST to generate LLVM IR.\n",
    "\n",
    "As a consequence, multiple frontends end up reimplementing significant pieces of\n",
    "infrastructure to support the need for these analyses and transformation. MLIR\n",
    "addresses this issue by being designed for extensibility. As such, there are few\n",
    "pre-defined instructions (*operations* in MLIR terminology) or types.\n",
    "\n",
    "## Interfacing with MLIR\n",
    "\n",
    "MLIR is designed to be a completely extensible infrastructure; there is no\n",
    "closed set of attributes (think: constant metadata), operations, or types. MLIR\n",
    "supports this extensibility with the concept of Dialects. Dialects provide a grouping \n",
    "mechanism for abstraction under a unique `namespace`.\n",
    "\n",
    "In MLIR, `Operations` are the core unit of abstraction and computation, similar in many \n",
    "ways to LLVM instructions. Operations can have application-specific semantics and can be \n",
    "used to represent all of the core IR structures in LLVM: instructions, globals (like \n",
    "functions), modules, etc.\n",
    "\n",
    "Here is the MLIR assembly for the Toy `transpose` operations:\n",
    "\n",
    "```mlir\n",
    "%t_tensor = \"toy.transpose\"(%tensor) {inplace = true} : (tensor<2x3xf64>) -> tensor<3x2xf64> loc(\"example/file/path\":12:1)\n",
    "```\n",
    "\n",
    "Let's break down the anatomy of this MLIR operation:\n",
    "\n",
    "-   `%t_tensor`\n",
    "\n",
    "    *   The name given to the result defined by this operation (which includes\n",
    "        [a prefixed sigil to avoid collisions](../../LangRef.md/#identifiers-and-keywords)).\n",
    "        An operation may define zero or more results (in the context of Toy, we\n",
    "        will limit ourselves to single-result operations), which are SSA values.\n",
    "        The name is used during parsing but is not persistent (e.g., it is not\n",
    "        tracked in the in-memory representation of the SSA value).\n",
    "\n",
    "-   `\"toy.transpose\"`\n",
    "\n",
    "    *   The name of the operation. It is expected to be a unique string, with\n",
    "        the namespace of the dialect prefixed before the \"`.`\". This can be read\n",
    "        as the `transpose` operation in the `toy` dialect.\n",
    "\n",
    "-   `(%tensor)`\n",
    "\n",
    "    *   A list of zero or more input operands (or arguments), which are SSA\n",
    "        values defined by other operations or referring to block arguments.\n",
    "\n",
    "-   `{ inplace = true }`\n",
    "\n",
    "    *   A dictionary of zero or more attributes, which are special operands that\n",
    "        are always constant. Here we define a boolean attribute named 'inplace'\n",
    "        that has a constant value of true.\n",
    "\n",
    "-   `(tensor<2x3xf64>) -> tensor<3x2xf64>`\n",
    "\n",
    "    *   This refers to the type of the operation in a functional form, spelling\n",
    "        the types of the arguments in parentheses and the type of the return\n",
    "        values afterward.\n",
    "\n",
    "-   `loc(\"example/file/path\":12:1)`\n",
    "\n",
    "    *   This is the location in the source code from which this operation\n",
    "        originated.\n",
    "\n",
    "Shown here is the general form of an operation. As described above,\n",
    "the set of operations in MLIR is extensible. Operations are modeled\n",
    "using a small set of concepts, enabling operations to be reasoned\n",
    "about and manipulated generically. These concepts are:\n",
    "\n",
    "-   A name for the operation.\n",
    "-   A list of SSA operand values.\n",
    "-   A list of [attributes](../../LangRef.md/#attributes).\n",
    "-   A list of [types](../../LangRef.md/#type-system) for result values.\n",
    "-   A [source location](../../Diagnostics.md/#source-locations) for debugging\n",
    "    purposes.\n",
    "-   A list of successors [blocks](../../LangRef.md/#blocks) (for branches,\n",
    "    mostly).\n",
    "-   A list of [regions](../../LangRef.md/#regions) (for structural operations\n",
    "    like functions).\n",
    "\n",
    "In MLIR, every operation has a mandatory source location associated with it.\n",
    "Contrary to LLVM, where debug info locations are metadata and can be dropped, in\n",
    "MLIR, the location is a core requirement, and APIs depend on and manipulate it.\n",
    "Dropping a location is thus an explicit choice which cannot happen by mistake.\n",
    "\n",
    "To provide an illustration: If a transformation replaces an operation by\n",
    "another, that new operation must still have a location attached. This makes it\n",
    "possible to track where that operation came from.\n",
    "\n",
    "It's worth noting that the mlir-opt tool - a tool for testing\n",
    "compiler passes - does not include locations in the output by default. The\n",
    "`-mlir-print-debuginfo` flag specifies to include locations. (Run `mlir-opt\n",
    "--help` for more options.)\n",
    "\n",
    "### Opaque API\n",
    "\n",
    "MLIR is designed to allow all IR elements, such as attributes, operations, and\n",
    "types, to be customized. At the same time, IR elements can always be reduced to\n",
    "the above fundamental concepts. This allows MLIR to parse, represent, and\n",
    "[round-trip](../../../getting_started/Glossary.md/#round-trip) IR for *any*\n",
    "operation. For example, we could place our Toy operation from above into an\n",
    "`.mlir` file and round-trip through *mlir-opt* without registering any `toy`\n",
    "related dialect:\n",
    "\n",
    "```mlir\n",
    "func @toy_func(%tensor: tensor<2x3xf64>) -> tensor<3x2xf64> {\n",
    "  %t_tensor = \"toy.transpose\"(%tensor) { inplace = true } : (tensor<2x3xf64>) -> tensor<3x2xf64>\n",
    "  return %t_tensor : tensor<3x2xf64>\n",
    "}\n",
    "```\n",
    "\n",
    "In the cases of unregistered attributes, operations, and types, MLIR will\n",
    "enforce some structural constraints (e.g. dominance, etc.), but otherwise they\n",
    "are completely opaque. For instance, MLIR has little information about whether\n",
    "an unregistered operation can operate on particular data types, how many\n",
    "operands it can take, or how many results it produces. This flexibility can be\n",
    "useful for bootstrapping purposes, but it is generally advised against in mature\n",
    "systems. Unregistered operations must be treated conservatively by\n",
    "transformations and analyses, and they are much harder to construct and\n",
    "manipulate.\n",
    "\n",
    "This handling can be observed by crafting what should be an invalid IR for Toy\n",
    "and seeing it round-trip without tripping the verifier:\n",
    "\n",
    "```mlir\n",
    "func @main() {\n",
    "  %0 = \"toy.print\"() : () -> tensor<2x3xf64>\n",
    "}\n",
    "```\n",
    "\n",
    "There are multiple problems here: the `toy.print` operation is not a terminator;\n",
    "it should take an operand; and it shouldn't return any values. In the next\n",
    "section, we will register our dialect and operations with MLIR, plug into the\n",
    "verifier, and add nicer APIs to manipulate our operations.\n",
    "\n",
    "\n",
    "## Defining Toy Operations\n",
    "\n",
    "Now that we have a `Toy` dialect, we can start defining the operations. This\n",
    "will allow for providing semantic information that the rest of the system can\n",
    "hook into. As an example, let's walk through the creation of a `toy.constant`\n",
    "operation. This operation will represent a constant value in the Toy language.\n",
    "\n",
    "```mlir\n",
    " %4 = \"toy.constant\"() {value = dense<1.0> : tensor<2x3xf64>} : () -> tensor<2x3xf64>\n",
    "```\n",
    "\n",
    "This operation takes zero operands, a dense elements attribute named `value` \n",
    "to represent the constant value, and returns a single result of RankedTensorType. \n",
    "An operation class inherits from the `Operation` class\n",
    "`mlir::Op` class which also takes some optional [*traits*](../../Traits.md) to\n",
    "customize its behavior. `Traits` are a mechanism with which we can inject\n",
    "additional behavior into an Operation, such as additional accessors,\n",
    "verification, and more. Let's look below at a possible definition for the\n",
    "constant operation that we have described above:\n",
    "\n",
    "``` python\n",
    "@irdl_op_definition\n",
    "class ConstantOp(Operation):\n",
    "    '''\n",
    "    Constant operation turns a literal into an SSA value. The data is attached\n",
    "    to the operation as an attribute. For example:\n",
    "\n",
    "      %0 = toy.constant dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]>\n",
    "                        : tensor<2x3xf64>\n",
    "    '''\n",
    "    name: str = \"toy.constant\"\n",
    "    value: OpAttr[DenseIntOrFPElementsAttr]\n",
    "    res: Annotated[OpResult, TensorTypeF64]\n",
    "\n",
    "    # TODO verify that the result and value type are equal\n",
    "\n",
    "    @staticmethod\n",
    "    def from_list(data: List[float], shape: List[int]):\n",
    "        value = DenseIntOrFPElementsAttr.tensor_from_list(data, f64, shape)\n",
    "\n",
    "        return ConstantOp.create(result_types=[value.type],\n",
    "                                 attributes={\"value\": value})\n",
    "\n",
    "    def verify_(self) -> None:\n",
    "        resultType = self.res.typ\n",
    "        value = self.value\n",
    "        if not isinstance(resultType, TensorType):\n",
    "            raise VerifyException(\"Expected result type to be `TensorTypeF64`\")\n",
    "\n",
    "        if not isinstance(value, DenseIntOrFPElementsAttr):\n",
    "            raise VerifyException(\n",
    "                \"Expected value type to be instance of `DenseIntOrFPElementsAttr`\"\n",
    "            )\n",
    "\n",
    "        if resultType.get_shape() != value.shape:\n",
    "            raise VerifyException(\n",
    "                \"Expected value and result to have the same shape\")\n",
    "```\n",
    "\n",
    "and we can register this operation in the `ToyDialect` initializer:\n",
    "\n",
    "```c++\n",
    "void ToyDialect::initialize() {\n",
    "  addOperations<ConstantOp>();\n",
    "}\n",
    "```\n",
    "\n",
    "### Op vs Operation: Using MLIR Operations\n",
    "\n",
    "Now that we have defined an operation, we will want to access and transform it.\n",
    "In MLIR, there are two main classes related to operations: `Operation` and `Op`.\n",
    "The `Operation` class is used to generically model all operations. It is\n",
    "'opaque', in the sense that it does not describe the properties of particular\n",
    "operations or types of operations. Instead, the `Operation` class provides a\n",
    "general API into an operation instance. On the other hand, each specific type of\n",
    "operation is represented by an `Op` derived class. For instance `ConstantOp`\n",
    "represents a operation with zero inputs, and one output, which is always set to\n",
    "the same value. `Op` derived classes act as smart pointer wrapper around a\n",
    "`Operation*`, provide operation-specific accessor methods, and type-safe\n",
    "properties of operations. This means that when we define our Toy operations, we\n",
    "are simply defining a clean, semantically useful interface for building and\n",
    "interfacing with the `Operation` class. This is why our `ConstantOp` defines no\n",
    "class fields; all of the data for this operation is stored in the referenced\n",
    "`Operation`. A side effect of this design is that we always pass around `Op`\n",
    "derived classes \"by-value\", instead of by reference or pointer (*passing by\n",
    "value* is a common idiom in MLIR and applies similarly to attributes, types,\n",
    "etc). Given a generic `Operation*` instance, we can always get a specific `Op`\n",
    "instance using LLVM's casting infrastructure:\n",
    "\n",
    "```c++\n",
    "void processConstantOp(mlir::Operation *operation) {\n",
    "  ConstantOp op = llvm::dyn_cast<ConstantOp>(operation);\n",
    "\n",
    "  // This operation is not an instance of `ConstantOp`.\n",
    "  if (!op)\n",
    "    return;\n",
    "\n",
    "  // Get the internal operation instance wrapped by the smart pointer.\n",
    "  mlir::Operation *internalOperation = op.getOperation();\n",
    "  assert(internalOperation == operation &&\n",
    "         \"these operation instances are the same\");\n",
    "}\n",
    "```\n",
    "\n",
    "### Using the Operation Definition Specification (ODS) Framework\n",
    "\n",
    "In addition to specializing the `mlir::Op` C++ template, MLIR also supports\n",
    "defining operations in a declarative manner. This is achieved via the\n",
    "[Operation Definition Specification](../../OpDefinitions.md) framework. Facts\n",
    "regarding an operation are specified concisely into a TableGen record, which\n",
    "will be expanded into an equivalent `mlir::Op` C++ template specialization at\n",
    "compile time. Using the ODS framework is the desired way for defining operations\n",
    "in MLIR given the simplicity, conciseness, and general stability in the face of\n",
    "C++ API changes.\n",
    "\n",
    "Lets see how to define the ODS equivalent of our ConstantOp:\n",
    "\n",
    "Operations in ODS are defined by inheriting from the `Op` class. To simplify our\n",
    "operation definitions, we will define a base class for operations in the Toy\n",
    "dialect.\n",
    "\n",
    "```tablegen\n",
    "// Base class for toy dialect operations. This operation inherits from the base\n",
    "// `Op` class in OpBase.td, and provides:\n",
    "//   * The parent dialect of the operation.\n",
    "//   * The mnemonic for the operation, or the name without the dialect prefix.\n",
    "//   * A list of traits for the operation.\n",
    "class Toy_Op<string mnemonic, list<Trait> traits = []> :\n",
    "    Op<Toy_Dialect, mnemonic, traits>;\n",
    "```\n",
    "\n",
    "With all of the preliminary pieces defined, we can begin to define the constant\n",
    "operation.\n",
    "\n",
    "We define a toy operation by inheriting from our base 'Toy_Op' class above. Here\n",
    "we provide the mnemonic and a list of traits for the operation. The\n",
    "[mnemonic](../../OpDefinitions.md/#operation-name) here matches the one given in\n",
    "`ConstantOp::getOperationName` without the dialect prefix; `toy.`. Missing here\n",
    "from our C++ definition are the `ZeroOperands` and `OneResult` traits; these\n",
    "will be automatically inferred based upon the `arguments` and `results` fields\n",
    "we define later.\n",
    "\n",
    "```tablegen\n",
    "def ConstantOp : Toy_Op<\"constant\"> {\n",
    "}\n",
    "```\n",
    "\n",
    "At this point you probably might want to know what the C++ code generated by\n",
    "TableGen looks like. Simply run the `mlir-tblgen` command with the\n",
    "`gen-op-decls` or the `gen-op-defs` action like so:\n",
    "\n",
    "```shell\n",
    "${build_root}/bin/mlir-tblgen -gen-op-defs ${mlir_src_root}/examples/toy/Ch2/include/toy/Ops.td -I ${mlir_src_root}/include/\n",
    "```\n",
    "\n",
    "Depending on the selected action, this will print either the `ConstantOp` class\n",
    "declaration or its implementation. Comparing this output to the hand-crafted\n",
    "implementation is incredibly useful when getting started with TableGen.\n",
    "\n",
    "#### Defining Arguments and Results\n",
    "\n",
    "With the shell of the operation defined, we can now provide the\n",
    "[inputs](../../OpDefinitions.md/#operation-arguments) and\n",
    "[outputs](../../OpDefinitions.md/#operation-results) to our operation. The\n",
    "inputs, or arguments, to an operation may be attributes or types for SSA operand\n",
    "values. The results correspond to a set of types for the values produced by the\n",
    "operation:\n",
    "\n",
    "```tablegen\n",
    "def ConstantOp : Toy_Op<\"constant\"> {\n",
    "  // The constant operation takes an attribute as the only input.\n",
    "  // `F64ElementsAttr` corresponds to a 64-bit floating-point ElementsAttr.\n",
    "  let arguments = (ins F64ElementsAttr:$value);\n",
    "\n",
    "  // The constant operation returns a single value of TensorType.\n",
    "  // F64Tensor corresponds to a 64-bit floating-point TensorType.\n",
    "  let results = (outs F64Tensor);\n",
    "}\n",
    "```\n",
    "\n",
    "By providing a name to the arguments or results, e.g. `$value`, ODS will\n",
    "automatically generate a matching accessor: `DenseElementsAttr\n",
    "ConstantOp::value()`.\n",
    "\n",
    "#### Adding Documentation\n",
    "\n",
    "The next step after defining the operation is to document it. Operations may\n",
    "provide\n",
    "[`summary` and `description`](../../OpDefinitions.md/#operation-documentation)\n",
    "fields to describe the semantics of the operation. This information is useful\n",
    "for users of the dialect and can even be used to auto-generate Markdown\n",
    "documents.\n",
    "\n",
    "```tablegen\n",
    "def ConstantOp : Toy_Op<\"constant\"> {\n",
    "  // Provide a summary and description for this operation. This can be used to\n",
    "  // auto-generate documentation of the operations within our dialect.\n",
    "  let summary = \"constant operation\";\n",
    "  let description = [{\n",
    "    Constant operation turns a literal into an SSA value. The data is attached\n",
    "    to the operation as an attribute. For example:\n",
    "\n",
    "      %0 = \"toy.constant\"()\n",
    "         { value = dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]> : tensor<2x3xf64> }\n",
    "        : () -> tensor<2x3xf64>\n",
    "  }];\n",
    "\n",
    "  // The constant operation takes an attribute as the only input.\n",
    "  // `F64ElementsAttr` corresponds to a 64-bit floating-point ElementsAttr.\n",
    "  let arguments = (ins F64ElementsAttr:$value);\n",
    "\n",
    "  // The generic call operation returns a single value of TensorType.\n",
    "  // F64Tensor corresponds to a 64-bit floating-point TensorType.\n",
    "  let results = (outs F64Tensor);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Verifying Operation Semantics\n",
    "\n",
    "At this point we've already covered a majority of the original C++ operation\n",
    "definition. The next piece to define is the verifier. Luckily, much like the\n",
    "named accessor, the ODS framework will automatically generate a lot of the\n",
    "necessary verification logic based upon the constraints we have given. This\n",
    "means that we don't need to verify the structure of the return type, or even the\n",
    "input attribute `value`. In many cases, additional verification is not even\n",
    "necessary for ODS operations. To add additional verification logic, an operation\n",
    "can override the [`verifier`](../../OpDefinitions.md/#custom-verifier-code)\n",
    "field. The `verifier` field allows for defining a C++ code blob that will be run\n",
    "as part of `ConstantOp::verify`. This blob can assume that all of the other\n",
    "invariants of the operation have already been verified:\n",
    "\n",
    "```tablegen\n",
    "def ConstantOp : Toy_Op<\"constant\"> {\n",
    "  // Provide a summary and description for this operation. This can be used to\n",
    "  // auto-generate documentation of the operations within our dialect.\n",
    "  let summary = \"constant operation\";\n",
    "  let description = [{\n",
    "    Constant operation turns a literal into an SSA value. The data is attached\n",
    "    to the operation as an attribute. For example:\n",
    "\n",
    "      %0 = \"toy.constant\"()\n",
    "         { value = dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]> : tensor<2x3xf64> }\n",
    "        : () -> tensor<2x3xf64>\n",
    "  }];\n",
    "\n",
    "  // The constant operation takes an attribute as the only input.\n",
    "  // `F64ElementsAttr` corresponds to a 64-bit floating-point ElementsAttr.\n",
    "  let arguments = (ins F64ElementsAttr:$value);\n",
    "\n",
    "  // The generic call operation returns a single value of TensorType.\n",
    "  // F64Tensor corresponds to a 64-bit floating-point TensorType.\n",
    "  let results = (outs F64Tensor);\n",
    "\n",
    "  // Add additional verification logic to the constant operation. Setting this bit\n",
    "  // to `1` will generate a `::mlir::LogicalResult verify()` declaration on the\n",
    "  // operation class that is called after ODS constructs have been verified, for\n",
    "  // example the types of arguments and results. We implement additional verification\n",
    "  // in the definition of this `verify` method in the C++ source file. \n",
    "  let hasVerifier = 1;\n",
    "}\n",
    "```\n",
    "\n",
    "#### Attaching `build` Methods\n",
    "\n",
    "The final missing component here from our original C++ example are the `build`\n",
    "methods. ODS can generate some simple build methods automatically, and in this\n",
    "case it will generate our first build method for us. For the rest, we define the\n",
    "[`builders`](../../OpDefinitions.md/#custom-builder-methods) field. This field\n",
    "takes a list of `OpBuilder` objects that take a string corresponding to a list\n",
    "of C++ parameters, as well as an optional code block that can be used to specify\n",
    "the implementation inline.\n",
    "\n",
    "```tablegen\n",
    "def ConstantOp : Toy_Op<\"constant\"> {\n",
    "  ...\n",
    "\n",
    "  // Add custom build methods for the constant operation. These methods populate\n",
    "  // the `state` that MLIR uses to create operations, i.e. these are used when\n",
    "  // using `builder.create<ConstantOp>(...)`.\n",
    "  let builders = [\n",
    "    // Build a constant with a given constant tensor value.\n",
    "    OpBuilder<(ins \"DenseElementsAttr\":$value), [{\n",
    "      // Call into an autogenerated `build` method.\n",
    "      build(builder, result, value.getType(), value);\n",
    "    }]>,\n",
    "\n",
    "    // Build a constant with a given constant floating-point value. This builder\n",
    "    // creates a declaration for `ConstantOp::build` with the given parameters.\n",
    "    OpBuilder<(ins \"double\":$value)>\n",
    "  ];\n",
    "}\n",
    "```\n",
    "\n",
    "## Defining a Toy Dialect\n",
    "\n",
    "xDSL provides a Python API to define dialects that are compatible with MLIR. To show its\n",
    "capabilities, we will define a new Toy dialect. This dialect will model the structure of \n",
    "the Toy language, as well as provide an easy avenue for high-level analysis and \n",
    "transformation.\n",
    "\n",
    "\n",
    "``` python\n",
    "Toy = Dialect([\n",
    "    ConstantOp, AddOp, FuncOp, GenericCallOp, PrintOp, MulOp, ReturnOp,\n",
    "    ReshapeOp, TransposeOp\n",
    "], [])\n",
    "```\n",
    "\n",
    "After the dialect has been defined, it can now be loaded into an MLIRContext:\n",
    "\n",
    "```python\n",
    "context = MLContext()\n",
    "\n",
    "context.register_dialect(Toy)\n",
    "```\n",
    "\n",
    "By default, an `MLContext` only loads the Builtin Dialect, which provides a few core IR\n",
    "components, meaning that other dialects, such as our `Toy` dialect, must be\n",
    "explicitly loaded.\n",
    "\n",
    "\n",
    "#### Specifying a Custom Assembly Format\n",
    "\n",
    "At this point we can generate our \"Toy IR\". For example, the following:\n",
    "\n",
    "```toy\n",
    "# User defined generic function that operates on unknown shaped arguments.\n",
    "def multiply_transpose(a, b) {\n",
    "  return transpose(a) * transpose(b);\n",
    "}\n",
    "\n",
    "def main() {\n",
    "  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];\n",
    "  var b<2, 3> = [1, 2, 3, 4, 5, 6];\n",
    "  var c = multiply_transpose(a, b);\n",
    "  var d = multiply_transpose(b, a);\n",
    "  print(d);\n",
    "}\n",
    "```\n",
    "\n",
    "Results in the following IR:\n",
    "\n",
    "```mlir\n",
    "module {\n",
    "  \"toy.func\"() ({\n",
    "  ^bb0(%arg0: tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":4:1), %arg1: tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":4:1)):\n",
    "    %0 = \"toy.transpose\"(%arg0) : (tensor<*xf64>) -> tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":5:10)\n",
    "    %1 = \"toy.transpose\"(%arg1) : (tensor<*xf64>) -> tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":5:25)\n",
    "    %2 = \"toy.mul\"(%0, %1) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":5:25)\n",
    "    \"toy.return\"(%2) : (tensor<*xf64>) -> () loc(\"test/Examples/Toy/Ch2/codegen.toy\":5:3)\n",
    "  }) {sym_name = \"multiply_transpose\", type = (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64>} : () -> () loc(\"test/Examples/Toy/Ch2/codegen.toy\":4:1)\n",
    "  \"toy.func\"() ({\n",
    "    %0 = \"toy.constant\"() {value = dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":9:17)\n",
    "    %1 = \"toy.reshape\"(%0) : (tensor<2x3xf64>) -> tensor<2x3xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":9:3)\n",
    "    %2 = \"toy.constant\"() {value = dense<[1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]> : tensor<6xf64>} : () -> tensor<6xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":10:17)\n",
    "    %3 = \"toy.reshape\"(%2) : (tensor<6xf64>) -> tensor<2x3xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":10:3)\n",
    "    %4 = \"toy.generic_call\"(%1, %3) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":11:11)\n",
    "    %5 = \"toy.generic_call\"(%3, %1) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc(\"test/Examples/Toy/Ch2/codegen.toy\":12:11)\n",
    "    \"toy.print\"(%5) : (tensor<*xf64>) -> () loc(\"test/Examples/Toy/Ch2/codegen.toy\":13:3)\n",
    "    \"toy.return\"() : () -> () loc(\"test/Examples/Toy/Ch2/codegen.toy\":8:1)\n",
    "  }) {sym_name = \"main\", type = () -> ()} : () -> () loc(\"test/Examples/Toy/Ch2/codegen.toy\":8:1)\n",
    "} loc(unknown)\n",
    "```\n",
    "\n",
    "One thing to notice here is that all of our Toy operations are printed using the\n",
    "generic assembly format. This format is the one shown when breaking down\n",
    "`toy.transpose` at the beginning of this chapter. MLIR allows for operations to\n",
    "define their own custom assembly format, either or imperatively via C++. Defining a custom \n",
    "assembly format allows for tailoring the generated IR into something a bit more readable \n",
    "by removing a lot of the fluff that is required by the generic format. Let's walk through \n",
    "an example of an operation format that we would like to simplify.\n",
    "\n",
    "This capability will soon be added to xDSL also, and will be interoperable with the MLIR\n",
    "format definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f24f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xdsl.ir import MLContext\n",
    "from xdsl.printer import Printer\n",
    "from toy import dialect as toy\n",
    "\n",
    "# MLContext, containing information about the registered dialects\n",
    "ctx = MLContext()\n",
    "\n",
    "ctx.register_dialect(toy.Toy)\n",
    "\n",
    "# Printer used to pretty-print MLIR data structures\n",
    "printer = Printer(target=Printer.Target.MLIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82230c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"toy.func\"() ({\n",
      "  ^0(%0 : tensor<*xf64>, %1 : tensor<*xf64>):\n",
      "    %2 = \"toy.transpose\"(%0) : (tensor<*xf64>) -> tensor<*xf64>\n",
      "    %3 = \"toy.transpose\"(%1) : (tensor<*xf64>) -> tensor<*xf64>\n",
      "    %4 = \"toy.mul\"(%2, %3) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64>\n",
      "    \"toy.return\"(%4) : (tensor<*xf64>) -> ()\n",
      "  }) {\"sym_name\" = \"multiply_transpose\", \"function_type\" = (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64>, \"sym_visibility\" = \"private\"} : () -> ()\n",
      "  \"toy.func\"() ({\n",
      "    %5 = \"toy.constant\"() {\"value\" = dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64>\n",
      "    %6 = \"toy.constant\"() {\"value\" = dense<[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]> : tensor<6xf64>} : () -> tensor<6xf64>\n",
      "    %7 = \"toy.reshape\"(%6) : (tensor<6xf64>) -> tensor<2x3xf64>\n",
      "    %8 = \"toy.generic_call\"(%5, %7) {\"callee\" = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64>\n",
      "    %9 = \"toy.generic_call\"(%7, %5) {\"callee\" = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64>\n",
      "    %10 = \"toy.generic_call\"(%7, %8) {\"callee\" = @multiply_transpose} : (tensor<2x3xf64>, tensor<*xf64>) -> tensor<*xf64>\n",
      "    %11 = \"toy.transpose\"(%5) : (tensor<2x3xf64>) -> tensor<3x2xf64>\n",
      "    %12 = \"toy.generic_call\"(%11, %8) {\"callee\" = @multiply_transpose} : (tensor<3x2xf64>, tensor<*xf64>) -> tensor<*xf64>\n",
      "    \"toy.return\"() : () -> ()\n",
      "  }) {\"sym_name\" = \"main\", \"function_type\" = () -> ()} : () -> ()\n",
      "}) : () -> ()\n"
     ]
    }
   ],
   "source": [
    "from xdsl.ir import SSAValue, BlockArgument, OpResult\n",
    "from xdsl.dialects.builtin import ModuleOp, f64, FunctionType\n",
    "from xdsl.builder import Builder\n",
    "\n",
    "\n",
    "@ModuleOp.from_region_or_ops\n",
    "@Builder.implicit_region\n",
    "def module_op():\n",
    "    unrankedf64TensorType = toy.UnrankedTensorType.from_type(f64)\n",
    "\n",
    "    multiply_transpose_type = FunctionType.from_lists(\n",
    "        [unrankedf64TensorType, unrankedf64TensorType],\n",
    "        [unrankedf64TensorType])\n",
    "\n",
    "    @Builder.implicit_region(multiply_transpose_type.inputs)\n",
    "    def multiply_transpose(args: tuple[BlockArgument, ...]) -> None:\n",
    "        a, b = args\n",
    "        a_t = toy.TransposeOp.from_input(a).res\n",
    "        b_t = toy.TransposeOp.from_input(b).res\n",
    "        prod = toy.MulOp.from_summands(a_t, b_t).res\n",
    "        toy.ReturnOp.from_input(prod)\n",
    "\n",
    "    def call_multiply_transpose(a: SSAValue, b: SSAValue) -> OpResult:\n",
    "        return toy.GenericCallOp.get(\"multiply_transpose\", [a, b],\n",
    "                                        [unrankedf64TensorType]).res[0]\n",
    "\n",
    "    main_type = FunctionType.from_lists([], [])\n",
    "\n",
    "    @Builder.implicit_region\n",
    "    def main() -> None:\n",
    "        a = toy.ConstantOp.from_list([1, 2, 3, 4, 5, 6], [2, 3]).res\n",
    "        b_0 = toy.ConstantOp.from_list([1, 2, 3, 4, 5, 6], [6]).res\n",
    "        b = toy.ReshapeOp.from_input(b_0, [2, 3]).res\n",
    "        c = call_multiply_transpose(a, b)\n",
    "        call_multiply_transpose(b, a)\n",
    "        call_multiply_transpose(b, c)\n",
    "        a_t = toy.TransposeOp.from_input(a).res\n",
    "        call_multiply_transpose(a_t, c)\n",
    "        toy.ReturnOp.from_input()\n",
    "\n",
    "    toy.FuncOp.from_region(\"multiply_transpose\",\n",
    "                            multiply_transpose_type,\n",
    "                            multiply_transpose,\n",
    "                            private=True)\n",
    "    toy.FuncOp.from_region(\"main\", main_type, main)\n",
    "\n",
    "printer.print(module_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2fe41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fda08596110b9a36b84966695987440228e4b909cf8fcaa87f570c5f74df3470"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
