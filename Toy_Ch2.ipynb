{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56341c66",
   "metadata": {},
   "source": [
    "# Chapter 2: Emitting Basic MLIR\n",
    "\n",
    "This is an xDSL version of the Toy compiler, as described in the [MLIR tutorial](https://mlir.llvm.org/docs/Tutorials/Toy/)\n",
    "\n",
    "This tutorial will be illustrated with a toy language that we’ll call “Toy” (naming is hard…). Toy is a tensor-based language that allows you to define functions, perform some math computation, and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0748a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8481b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"toy.func\"() ({\n",
      "  ^0(%0 : tensor<1xf64>, %1 : tensor<1xf64>):\n",
      "    %2 = \"toy.transpose\"(%0) : (tensor<1xf64>) -> tensor<1xf64>\n",
      "    %3 = \"toy.transpose\"(%1) : (tensor<1xf64>) -> tensor<1xf64>\n",
      "    %4 = \"toy.mul\"(%2, %3) : (tensor<1xf64>, tensor<1xf64>) -> f64\n",
      "    \"toy.return\"(%4) : (f64) -> ()\n",
      "  }) {\"sym_name\" = #symbol_name<\"multiply_transpose\">, \"function_type\" = (tensor<1xf64>, tensor<1xf64>) -> tensor<1xf64>} : () -> ()\n",
      "  \"toy.func\"() ({\n",
      "    %5 = \"toy.constant\"() {\"value\" = dense<[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]> : tensor<2x3xf64>} : () -> tensor<2x3xf64>\n",
      "    %6 = \"toy.reshape\"(%5) : (tensor<2x3xf64>) -> tensor<2x3xf64>\n",
      "    %7 = \"toy.constant\"() {\"value\" = dense<[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]> : tensor<6xf64>} : () -> tensor<6xf64>\n",
      "    %8 = \"toy.reshape\"(%7) : (tensor<6xf64>) -> tensor<2x3xf64>\n",
      "    %9 = \"toy.generic_call\"(%6, %8) {\"callee\" = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<2x3xf64>\n",
      "    %10 = \"toy.generic_call\"(%8, %6) {\"callee\" = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<2x3xf64>\n",
      "    \"toy.print\"(%10) : (tensor<2x3xf64>) -> ()\n",
      "    \"toy.return\"(%10) : (tensor<2x3xf64>) -> ()\n",
      "  }) {\"sym_name\" = #symbol_name<\"main\">, \"function_type\" = () -> ()} : () -> ()\n",
      "}) : () -> ()\n"
     ]
    }
   ],
   "source": [
    "from xdsl import *\n",
    "from xdsl.ir import *\n",
    "from xdsl.irdl import *\n",
    "from xdsl.dialects.func import *\n",
    "from xdsl.dialects.arith import *\n",
    "from xdsl.dialects.builtin import *\n",
    "from xdsl.parser import *\n",
    "from xdsl.printer import *\n",
    "from xdsl.util import *\n",
    "\n",
    "# MLContext, containing information about the registered dialects\n",
    "context = MLContext()\n",
    "\n",
    "# Some useful dialects\n",
    "arith = Arith(context)\n",
    "func = Func(context)\n",
    "builtin = Builtin(context)\n",
    "\n",
    "# Printer used to pretty-print MLIR data structures\n",
    "printer = Printer(target=Printer.Target.MLIR)\n",
    "\n",
    "@dataclass\n",
    "class Toy:\n",
    "    ctx: MLContext\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.ctx.register_op(ConstantOp)\n",
    "        # self.ctx.register_op(Call)\n",
    "        # self.ctx.register_op(Return)\n",
    "\n",
    "TensorTypeF64 = TensorType.from_type_and_list(f64)\n",
    "\n",
    "@irdl_op_definition\n",
    "class ConstantOp(Operation):\n",
    "    '''\n",
    "    Constant operation turns a literal into an SSA value. The data is attached\n",
    "    to the operation as an attribute. For example:\n",
    "\n",
    "    ```mlir\n",
    "      %0 = toy.constant dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]>\n",
    "                        : tensor<2x3xf64>\n",
    "    ```\n",
    "    '''\n",
    "    name: str = \"toy.constant\"\n",
    "    result = ResultDef(AnyAttr()) #TensorType[F64])\n",
    "    value = AttributeDef(AnyAttr()) #F64ElementsAttr\n",
    "\n",
    "    # TODO verify that the result and value type are equal\n",
    "\n",
    "    @staticmethod\n",
    "    def from_list(data: List[float], shape: List[int] = []):\n",
    "        value = DenseIntOrFPElementsAttr.tensor_from_list(data, f64, shape)\n",
    "        return ConstantOp.create(result_types=[value.type], attributes={\"value\": value})\n",
    "\n",
    "\n",
    "AddOpT = TypeVar('AddOpT', bound='AddOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class AddOp(Operation):\n",
    "    '''\n",
    "    The \"add\" operation performs element-wise addition between two tensors.\n",
    "    The shapes of the tensor operands are expected to match.\n",
    "    '''\n",
    "    name: str = 'toy.add'\n",
    "    arguments = VarOperandDef(AnyAttr())\n",
    "    result = ResultDef(AnyAttr()) # F64ElementsAttr\n",
    "\n",
    "    @classmethod\n",
    "    def from_summands(cls: type[AddOpT], lhs: OpResult, rhs: SSAValue) -> AddOpT:\n",
    "        return super().create(result_types=[lhs.typ.element_type], operands=[lhs, rhs])\n",
    "\n",
    "\n",
    "@irdl_op_definition\n",
    "class FuncOp(Operation):\n",
    "    '''\n",
    "    The \"toy.func\" operation represents a user defined function. These are\n",
    "    callable SSA-region operations that contain toy computations.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```mlir\n",
    "    toy.func @main() {\n",
    "      %0 = toy.constant dense<5.500000e+00> : tensor<f64>\n",
    "      %1 = toy.reshape(%0 : tensor<f64>) to tensor<2x2xf64>\n",
    "      toy.print %1 : tensor<2x2xf64>\n",
    "      toy.return\n",
    "    }\n",
    "    ```\n",
    "    '''\n",
    "    name = 'toy.func'\n",
    "    body = RegionDef()\n",
    "    sym_name = AttributeDef(SymbolNameAttr)\n",
    "    function_type = AttributeDef(FunctionType)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_region(name: str, ftype: FunctionType, region: Region):\n",
    "        return FuncOp.create(result_types=[ftype.outputs], attributes={\n",
    "            \"sym_name\": SymbolNameAttr.from_str(name),\n",
    "            \"function_type\": ftype\n",
    "        }, regions=[region])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_callable(name: str, input_types: List[Attribute],\n",
    "                      return_types: List[Attribute],\n",
    "                      func: Block.BlockCallback):\n",
    "        type_attr = FunctionType.from_lists(input_types, return_types)\n",
    "        op = FuncOp.build(attributes={\n",
    "            \"sym_name\": name,\n",
    "            \"function_type\": type_attr,\n",
    "        },\n",
    "                          regions=[\n",
    "                              Region.from_block_list(\n",
    "                                  [Block.from_callable(input_types, func)])\n",
    "                          ])\n",
    "        return op\n",
    "\n",
    "COpT = TypeVar('COpT', bound='GenericCallOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class GenericCallOp(Operation):\n",
    "    name: str = \"toy.generic_call\"\n",
    "    arguments = VarOperandDef(AnyAttr())\n",
    "    callee = AttributeDef(FlatSymbolRefAttr)\n",
    "\n",
    "    # Note: naming this results triggers an ArgumentError\n",
    "    res = VarResultDef(AnyAttr())\n",
    "    # TODO how do we verify that the types are correct?\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls: type[COpT], callee: Union[str, FlatSymbolRefAttr],\n",
    "            operands: List[Union[SSAValue, Operation]],\n",
    "            return_types: List[Attribute]) -> COpT:\n",
    "        if isinstance(callee, str):\n",
    "            callee = FlatSymbolRefAttr.from_str(callee)\n",
    "        \n",
    "        return super().create(operands=operands, result_types=return_types, attributes={\"callee\": callee})\n",
    "\n",
    "\n",
    "MulOpT = TypeVar('MulOpT', bound='MulOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class MulOp(Operation):\n",
    "    '''\n",
    "    The \"mul\" operation performs element-wise multiplication between two\n",
    "    tensors. The shapes of the tensor operands are expected to match.\n",
    "    '''\n",
    "    name: str = 'toy.mul'\n",
    "    arguments = VarOperandDef(AnyAttr())\n",
    "    result = ResultDef(AnyAttr()) # F64ElementsAttr\n",
    "\n",
    "    @classmethod\n",
    "    def from_summands(cls: type[MulOpT], lhs: OpResult, rhs: SSAValue) -> MulOpT:\n",
    "        return super().create(result_types=[lhs.typ.element_type], operands=[lhs, rhs])\n",
    "\n",
    "\n",
    "\n",
    "PrintOpT = TypeVar('PrintOpT', bound='PrintOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class PrintOp(Operation):\n",
    "    '''\n",
    "    The \"print\" builtin operation prints a given input tensor, and produces\n",
    "    no results.\n",
    "    '''\n",
    "    name: str = 'toy.print'\n",
    "    arguments = VarOperandDef(AnyAttr())\n",
    "\n",
    "    @classmethod\n",
    "    def from_input(cls: type[PrintOpT], input: SSAValue) -> PrintOpT:\n",
    "        return super().create(operands=[input])\n",
    "\n",
    "\n",
    "ReturnOpT = TypeVar('ReturnOpT', bound='ReturnOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class ReturnOp(Operation):\n",
    "    '''\n",
    "    The \"return\" operation represents a return operation within a function.\n",
    "    The operation takes an optional tensor operand and produces no results.\n",
    "    The operand type must match the signature of the function that contains\n",
    "    the operation. For example:\n",
    "\n",
    "    ```mlir\n",
    "      func @foo() -> tensor<2xf64> {\n",
    "        ...\n",
    "        toy.return %0 : tensor<2xf64>\n",
    "      }\n",
    "    ```\n",
    "    '''\n",
    "    name: str = 'toy.return'\n",
    "    arguments = OptOperandDef(AnyAttr())\n",
    "\n",
    "    @classmethod\n",
    "    def from_input(cls: type[ReturnOpT], input: SSAValue) -> ReturnOpT:\n",
    "        return super().create(operands=[input])\n",
    "\n",
    "\n",
    "ROpT = TypeVar('ROpT', bound='ReshapeOp')\n",
    "\n",
    "@irdl_op_definition\n",
    "class ReshapeOp(Operation):\n",
    "    '''\n",
    "    Reshape operation is transforming its input tensor into a new tensor with\n",
    "    the same number of elements but different shapes. For example:\n",
    "\n",
    "    ```mlir\n",
    "       %0 = toy.reshape (%arg1 : tensor<10xf64>) to tensor<5x2xf64>\n",
    "    ```\n",
    "    '''\n",
    "    name: str = 'toy.reshape'\n",
    "    arguments = VarOperandDef(AnyAttr())\n",
    "    # We expect that the reshape operation returns a statically shaped tensor.\n",
    "    result = ResultDef(AnyAttr()) # F64ElementsAttr \n",
    "\n",
    "    @classmethod\n",
    "    def from_input(cls: type[ROpT], input: SSAValue, shape: List[int]) -> ROpT:\n",
    "        t = AnyTensorType.from_type_and_list(input.typ.element_type, shape)\n",
    "        return super().create(result_types=[t], operands=[input])\n",
    "\n",
    "\n",
    "@irdl_op_definition\n",
    "class TransposeOp(Operation):\n",
    "    name: str = 'toy.transpose'\n",
    "    arguments = OperandDef(AnyAttr())\n",
    "    result = ResultDef(AnyAttr()) # F64ElementsAttr\n",
    "\n",
    "    @staticmethod\n",
    "    def from_input(input: SSAValue):\n",
    "        input_type = input.typ\n",
    "        if not isinstance(input_type, TensorType):\n",
    "            assert False, f'{input_type}: {type(input_type)}'\n",
    "        \n",
    "        output_type = TensorType.from_type_and_list(input_type.element_type, list(reversed(input_type.shape.data)))\n",
    "\n",
    "        return TransposeOp.create(operands=[input], result_types=[output_type])\n",
    "\n",
    "\n",
    "def func_body(arg0: SSAValue, arg1: SSAValue) -> List[Operation]:\n",
    "    f0 = TransposeOp.from_input(arg0)\n",
    "    f1 = TransposeOp.from_input(arg1)\n",
    "    f2 = MulOp.from_summands(f0.results[0], f1.results[0])\n",
    "    f3 = ReturnOp.from_input(f2.results[0])\n",
    "    return [f0, f1, f2, f3]\n",
    "\n",
    "def main_body() -> List[Operation]:\n",
    "    m0 = ConstantOp.from_list([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])\n",
    "    m1 = ReshapeOp.from_input(m0.results[0], [2,3])\n",
    "    m2 = ConstantOp.from_list([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [])\n",
    "    m3 = ReshapeOp.from_input(m2.results[0], [2,3])\n",
    "    m4 = GenericCallOp.get('multiply_transpose', [m1.results[0], m3.results[0]], [TensorType.from_type_and_list(f64, [2, 3])])\n",
    "    m5 = GenericCallOp.get('multiply_transpose', [m3.results[0], m1.results[0]], [TensorType.from_type_and_list(f64, [2, 3])])\n",
    "    m6 = PrintOp.from_input(m5.results[0])\n",
    "    m7 = ReturnOp.from_input(m5.results[0])\n",
    "    return [\n",
    "        m0, m1, m2, m3, m4, m5, m6, m7\n",
    "    ]\n",
    "\n",
    "multiply_transpose = FuncOp.from_callable('multiply_transpose', [TensorTypeF64, TensorTypeF64], [TensorTypeF64], func_body)\n",
    "main = FuncOp.from_callable('main', [], [], main_body)\n",
    "\n",
    "module = ModuleOp.from_region_or_ops([multiply_transpose, main])\n",
    "\n",
    "printer.print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const_op = ConstantOp.from_list([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])\n",
    "printer.print_op(const_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %0 : !tensor<[2 : !index, 3 : !index], !f64> = toy.constant() [\"value\" = !dense<!tensor<[2 : !index, 3 : !index], !f64>, [1.0 : !f64, 2.0 : !f64, 3.0 : !f64, 4.0 : !f64, 5.0 : !f64, 6.0 : !f64]>]\n",
    "# %0 = \"toy.constant\"() {\"value\" = dense<[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]> : tensor<2x3x!f64>} : () -> tensor<2x3x!f64>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56359230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!int<42>"
     ]
    }
   ],
   "source": [
    "my_int = IntAttr.from_int(42)\n",
    "printer.print_attribute(my_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1c094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ast_toy = Path() / 'toy' / 'ast.toy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28a85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# RUN: toyc-ch1 %s -emit=ast 2>&1 | FileCheck %s\n",
      "\n",
      "# User defined generic function that operates on unknown shaped arguments.\n",
      "def multiply_transpose(a, b) {\n",
      "  return transpose(a) * transpose(b);\n",
      "}\n",
      "\n",
      "def main() {\n",
      "  # Define a variable `a` with shape <2, 3>, initialized with the literal value.\n",
      "  # The shape is inferred from the supplied literal.\n",
      "  var a = [[1, 2, 3], [4, 5, 6]];\n",
      "  # b is identical to a, the literal array is implicitly reshaped: defining new\n",
      "  # variables is the way to reshape arrays (element count in literal must match\n",
      "  # the size of specified shape).\n",
      "  var b<2, 3> = [1, 2, 3, 4, 5, 6];\n",
      "\n",
      "  # This call will specialize `multiply_transpose` with <2, 3> for both\n",
      "  # arguments and deduce a return type of <2, 2> in initialization of `c`.\n",
      "  var c = multiply_transpose(a, b);\n",
      "  # A second call to `multiply_transpose` with <2, 3> for both arguments will\n",
      "  # reuse the previously specialized and inferred version and return `<2, 2>`\n",
      "  var d = multiply_transpose(b, a);\n",
      "  # A new call with `<2, 2>` for both dimension will trigger another\n",
      "  # specialization of `multiply_transpose`.\n",
      "  var e = multiply_transpose(b, c);\n",
      "  # Finally, calling into `multiply_transpose` with incompatible shape will\n",
      "  # trigger a shape inference error.\n",
      "  var f = multiply_transpose(transpose(a), c);\n",
      "}\n",
      "\n",
      "\n",
      "# CHECK: Module:\n",
      "# CHECK-NEXT:     Function\n",
      "# CHECK-NEXT:       Proto 'multiply_transpose' @{{.*}}ast.toy:4:1\n",
      "# CHECK-NEXT:       Params: [a, b]\n",
      "# CHECK-NEXT:       Block {\n",
      "# CHECK-NEXT:         Return\n",
      "# CHECK-NEXT:           BinOp: * @{{.*}}ast.toy:5:25\n",
      "# CHECK-NEXT:             Call 'transpose' [ @{{.*}}ast.toy:5:10\n",
      "# CHECK-NEXT:               var: a @{{.*}}ast.toy:5:20\n",
      "# CHECK-NEXT:             ]\n",
      "# CHECK-NEXT:             Call 'transpose' [ @{{.*}}ast.toy:5:25\n",
      "# CHECK-NEXT:               var: b @{{.*}}ast.toy:5:35\n",
      "# CHECK-NEXT:             ]\n",
      "# CHECK-NEXT:       } // Block\n",
      "# CHECK-NEXT:     Function\n",
      "# CHECK-NEXT:       Proto 'main' @{{.*}}ast.toy:8:1\n",
      "# CHECK-NEXT:       Params: []\n",
      "# CHECK-NEXT:       Block {\n",
      "# CHECK-NEXT:         VarDecl a<> @{{.*}}ast.toy:11:3\n",
      "# CHECK-NEXT:           Literal: <2, 3>[ <3>[ 1.000000e+00, 2.000000e+00, 3.000000e+00], <3>[ 4.000000e+00, 5.000000e+00, 6.000000e+00]] @{{.*}}ast.toy:11:11\n",
      "# CHECK-NEXT:         VarDecl b<2, 3> @{{.*}}ast.toy:15:3\n",
      "# CHECK-NEXT:           Literal: <6>[ 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00] @{{.*}}ast.toy:15:17\n",
      "# CHECK-NEXT:         VarDecl c<> @{{.*}}ast.toy:19:3\n",
      "# CHECK-NEXT:           Call 'multiply_transpose' [ @{{.*}}ast.toy:19:11\n",
      "# CHECK-NEXT:             var: a @{{.*}}ast.toy:19:30\n",
      "# CHECK-NEXT:             var: b @{{.*}}ast.toy:19:33\n",
      "# CHECK-NEXT:           ]\n",
      "# CHECK-NEXT:         VarDecl d<> @{{.*}}ast.toy:22:3\n",
      "# CHECK-NEXT:           Call 'multiply_transpose' [ @{{.*}}ast.toy:22:11\n",
      "# CHECK-NEXT:             var: b @{{.*}}ast.toy:22:30\n",
      "# CHECK-NEXT:             var: a @{{.*}}ast.toy:22:33\n",
      "# CHECK-NEXT:           ]\n",
      "# CHECK-NEXT:         VarDecl e<> @{{.*}}ast.toy:25:3\n",
      "# CHECK-NEXT:           Call 'multiply_transpose' [ @{{.*}}ast.toy:25:11\n",
      "# CHECK-NEXT:             var: b @{{.*}}ast.toy:25:30\n",
      "# CHECK-NEXT:             var: c @{{.*}}ast.toy:25:33\n",
      "# CHECK-NEXT:           ]\n",
      "# CHECK-NEXT:         VarDecl f<> @{{.*}}ast.toy:28:3\n",
      "# CHECK-NEXT:           Call 'multiply_transpose' [ @{{.*}}ast.toy:28:11\n",
      "# CHECK-NEXT:             Call 'transpose' [ @{{.*}}ast.toy:28:30\n",
      "# CHECK-NEXT:               var: a @{{.*}}ast.toy:28:40\n",
      "# CHECK-NEXT:             ]\n",
      "# CHECK-NEXT:             var: c @{{.*}}ast.toy:28:44\n",
      "# CHECK-NEXT:           ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(ast_toy, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd72492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:\n",
      "  Function \n",
      "    Proto 'multiply_transpose' toy/ast.toy:4:1'\n",
      "    Params: [a, b]\n",
      "    Block {\n",
      "      Return\n",
      "        BinOp: * toy/ast.toy:5:25\n",
      "          Call 'transpose' [ toy/ast.toy:5:10\n",
      "            var: a toy/ast.toy:5:20\n",
      "          ]\n",
      "          Call 'transpose' [ toy/ast.toy:5:25\n",
      "            var: b toy/ast.toy:5:35\n",
      "          ]\n",
      "    } // Block\n",
      "  Function \n",
      "    Proto 'main' toy/ast.toy:8:1'\n",
      "    Params: []\n",
      "    Block {\n",
      "      VarDecl a<> toy/ast.toy:11:3\n",
      "        Literal: <2, 3>[ <3>[ 1.000000e+00, 2.000000e+00, 3.000000e+00], <3>[ 4.000000e+00, 5.000000e+00, 6.000000e+00]] toy/ast.toy:11:11\n",
      "      VarDecl b<2, 3> toy/ast.toy:15:3\n",
      "        Literal: <6>[ 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00] toy/ast.toy:15:17\n",
      "      VarDecl c<> toy/ast.toy:19:3\n",
      "        Call 'multiply_transpose' [ toy/ast.toy:19:11\n",
      "          var: a toy/ast.toy:19:30\n",
      "          var: b toy/ast.toy:19:33\n",
      "        ]\n",
      "      VarDecl d<> toy/ast.toy:22:3\n",
      "        Call 'multiply_transpose' [ toy/ast.toy:22:11\n",
      "          var: b toy/ast.toy:22:30\n",
      "          var: a toy/ast.toy:22:33\n",
      "        ]\n",
      "      VarDecl e<> toy/ast.toy:25:3\n",
      "        Call 'multiply_transpose' [ toy/ast.toy:25:11\n",
      "          var: b toy/ast.toy:25:30\n",
      "          var: c toy/ast.toy:25:33\n",
      "        ]\n",
      "      VarDecl f<> toy/ast.toy:28:3\n",
      "        Call 'multiply_transpose' [ toy/ast.toy:28:11\n",
      "          Call 'transpose' [ toy/ast.toy:28:30\n",
      "            var: a toy/ast.toy:28:40\n",
      "          ]\n",
      "          var: c toy/ast.toy:28:44\n",
      "        ]\n",
      "    } // Block\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from toy.Parser import Parser\n",
    "\n",
    "with open(ast_toy, 'r') as f:\n",
    "    parser = Parser(ast_toy, f.read())\n",
    "\n",
    "print(parser.parseModule().dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b149b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fda08596110b9a36b84966695987440228e4b909cf8fcaa87f570c5f74df3470"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
